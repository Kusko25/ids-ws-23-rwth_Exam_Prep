{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS Cheatsheet for Exam Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Preprocessing\n",
    "\n",
    "|Mean|Variance|Standard Deviation|Standard Score|Covariance|Correlation|\n",
    "|:------|:------|:------|:------|:------|:------|\n",
    "|$mean(X) = \\frac{sum(X)}{len(X)}$|$var(a)=\\frac{\\sum_{a_i \\in a}(a_i - \\bar{a})^2}{len(a)-1}$|$std(V) = \\sqrt{var(V)}$|$stsc(x) = \\frac{x-mean(X)}{std(X)}$|$\\frac{1}{n-1}\\sum_{i=1}^n ((a_i-\\bar{a})*(b_i-\\bar{b}))$|$\\frac{cov(a,b)}{std(a)*std(b)}$|\n",
    "\n",
    "|Entropy|Median (In this case)|1st/3rd Quartile|\n",
    "|:------|:------|:------|\n",
    "|$H(t) = - \\sum_{i=1}^I(P(t=i)*log_s(P(t=i)))$|Middle element if odd, avg of 2 middle if even|Median of lower/upper half after removing median|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions Neural Networks\n",
    "|Identity|Logistic/Sigmoid|Step|\n",
    "|------|------|------|\n",
    "|$f(x)=x$|$\\frac{1}{1+e^{-x}}$|$x \\geq 0 \\rightarrow 1$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "<table><tr><td colspan=\"2\" rowspan=\"2\"></td><td colspan=\"2\">predicted</td></tr><tr><td>True</td><td>False</td></tr><tr><td rowspan=\"2\">target</td><td>True</td><td>True Positive</td><td>False Negative</td></tr><tr><td>False</td><td>False Positive</td><td>True Negative</td></tr></table>\n",
    " \n",
    " --------------------------------------------------------\n",
    "\n",
    "|Recall|Specificity|Precision|\n",
    "|------|------|------|\n",
    "|$\\frac{TP}{TP+FN}$|$\\frac{TN}{TN+FP}$|$\\frac{TP}{TP+FP}$|\n",
    "|sensitivity, recall, hit rate, or true positive rate|specificity, selectivity or true negative rate|precision or positive predictive value|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics\n",
    "\n",
    "|Euclidean|Manhattan|Cosine|Dot Product|Magnitude|\n",
    "|------|------|------|------|------|\n",
    "|$d(x,y) = \\sqrt{\\sum_{i=1}^n (x_i-y_i)^2}$|$d(x,y) = \\sum_{i=1}^n \\|x_i-y_i\\|$|$d(x,y) = \\frac{x \\cdot y}{\\|\\|x\\|\\| \\cdot \\|\\|y\\|\\|}$|$x \\cdot y = \\sum_{i=1}^n x_i \\cdot y_i$|$\\|\\|x\\|\\| = \\sqrt{\\sum_{i=1}^n x_i^2}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Metrics\n",
    "|Mean Absolute Error|Mean Squared Error|Root Mean Squared Error|Mean Absolute Percentage Error|Mean Percentage Error|\n",
    "|------|------|------|------|------|\n",
    "|$MAE = \\frac{1}{n}\\sum_{i=1}^n \\|y_i-\\hat{y}_i\\|$|$MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat{y}_i)^2$|$RMSE = \\sqrt{MSE}$|$MAPE = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\|y_i-\\hat{y}_i\\|}{y_i}$|$MPE = \\frac{1}{n}\\sum_{i=1}^n \\frac{y_i-\\hat{y}_i}{y_i}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Stability Index\n",
    "$f_A = $ fraction of class A in the dataset  \n",
    "$f_A' = $ fraction of class A in the dataset after change.  \n",
    "System-Stability-Index for class $A$ $ssi_C(A) = (f_A - f_A') \\cdot \\log_2(\\frac{f_A}{f_A'})$  \n",
    "System-Stability-Index $ssi(\\text{System}) = \\sum_{i=1}^n ssi_C(A_i)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "|-|K-Means|DBSCAN|Hierarchical|\n",
    "|:------|------|------|------|\n",
    "|**Initial**|K-Randomly picked centroids|All points with minPts in $\\epsilon$-neighborhood set as core points|Each point is a cluster|\n",
    "|**Iteration**|Assign points to nearest centroid, average points to get new centroid|-|Merge closest clusters|\n",
    "|**Termination**|Centroids don't (or minimal) change|Core points within $\\epsilon$-neighborhood are clustered, non-core with core point in $\\epsilon$-neighborhood are assigned to closest cluster, rest are noise|Target number of clusters is reached or no more clusters can be merged without exceeding threshold|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Itemsets\n",
    "\n",
    "### Metrics\n",
    "|Support|Confidence|Lift|Conviction|\n",
    "|------|------|------|------|\n",
    "|$supp(X) = \\frac{count(X)}{N}$|$conf(X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)}$|$lift(X \\rightarrow Y) = \\frac{conf(X \\rightarrow Y)}{supp(Y)} = \\frac{supp(X \\cup Y)}{supp(X)*supp(Y)}$|$conv(X \\rightarrow Y) = \\frac{1-supp(Y)}{1-conf(X \\rightarrow Y)}$|\n",
    "|Support shows how relevant this itemset is|Confidence shows how likely Y is bought if X is bought|Lift shows how likely Y is bought if X is bought, while controlling for how popular Y is|Conviction shows how much X is bought without Y|\n",
    "### Properties\n",
    "|Frequent|Closed|Maximal Frequent|\n",
    "|------|------|------|\n",
    "|Set has more than min support|All supersets have lower support|Set is frequent, closed and no superset is frequent|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Petri Nets\n",
    "\n",
    "|Situation|Behavior|\n",
    "|------|------|\n",
    "|All output places are empty|Outputs +1 token, produced+=outputs|\n",
    "|Some output places have a token|Outputs +1 token, produced+=outputs positions can have multiple tokens|\n",
    "|------|------|\n",
    "|All input places have a token|Inputs -1 token, consumed+=inputs|\n",
    "|Some input places have a token|Produce missing_tokens on empty inputs, Inputs -1 token, consumed+=inputs, missing+=missing_tokens, produced not affected|\n",
    "|------|------|\n",
    "|Start of trace|start_positions +1 token produced+=start_positions|\n",
    "|End of trace|end_positions -1 token consumed+=end_positions missing+=amount_missing_on_end|\n",
    "|------|------|\n",
    "|Tokens remaining after trace|remaining+=tokens|\n",
    "\n",
    "### Fitness\n",
    "$fitness = \\frac{1}{2}(1-\\frac{missing}{consumed})+\\frac{1}{2}(1-\\frac{remaining}{produced})$\n",
    "### Rules\n",
    "$remaining = produced + missing - consumed$\n",
    "\n",
    "$produced + missing \\geq consumed \\geq missing$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inductive Miner\n",
    "\n",
    "Maximize # of partitions.\n",
    "### Exclusive Choice Cut $X$\n",
    "No connection between partitions\n",
    "\n",
    "### Sequence Cut $\\rightarrow$\n",
    "Partitions are acyclic\n",
    "\n",
    "### Parallel Cut $\\land$\n",
    "Each partition has a start and end. Every node in partition A can be reached from every node in partition B and vice versa.\n",
    "\n",
    "### Loop Cut $\\circlearrowleft$\n",
    "All start and end nodes are in the same partition. Every outgoing edge from that partition comes from an end node and every incoming edge goes to a start node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "\n",
    "|Term Frequency|Set Frequency|\n",
    "|------|------|\n",
    "|$tf(t,d) = \\sum_{t' \\in d} [t' = t]$|$sf(t,d) = [t \\in d]$|\n",
    "|**Inverse Document Frequency**|**Term Frequency-Inverse Document Frequency**|\n",
    "|$idf(t, C) = \\log_2(\\frac{\\|C\\|}{\\sum_{d \\in C} sf(t,d)})$|$tfidf(t,d,C) = tf(t,d) \\cdot idf(t,C)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Responsible Data Science\n",
    "\n",
    "### Discrimination\n",
    "Base rule: $b = A \\rightarrow B$, where $B$ is the outcome and $A$ is the set of attributes, without the attribute $C$ that is being tested for discrimination.  \n",
    "PD rule: $r = A, C \\rightarrow B$, where $C$ is the attribute being tested for discrimination.\n",
    "\n",
    "|Support|Confidence|Extended Lift|\n",
    "|------|------|------|\n",
    "|$supp(X \\rightarrow Y) = \\frac{count(X \\cup Y)}{N}$|$conf(X \\rightarrow Y) = \\frac{supp(X \\cup Y)}{supp(X)}$|$elift(b,r) = \\frac{conf(r)}{conf(b)}$|\n",
    "|**$\\alpha$-discrimination**|**Discrimination**|\n",
    "|$elift(b,r) \\geq \\alpha$|$\\frac{\\text{unprotected classified positive}}{\\text{unprotected}} - \\frac{\\text{protected classified positive}}{\\text{protected}}$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data\n",
    "\n",
    "### MapReduce\n",
    "1. Loads of values\n",
    "2. Map assigns a key to each value\n",
    "3. Shuffle sorts and groups values by key. All values with the same key are sent to the same reducer.\n",
    "4. Reduce processes the values with the given key. And returns a new key-value pair.\n",
    "\n",
    "### Moving K-Means\n",
    "1. Initialize with dampening factor $\\alpha$, number of clusters $k$, cluster weights $w_{1..k}$, cluster centers $c_{1..k}$ and threshold $\\delta$.\n",
    "2. New batch of points $p_{1..n}$ arrives with weights $pw_{1..n}$.\n",
    "3. For each point, calculate the distance to each cluster center and assign it to the closest cluster.\n",
    "4. Compute new center by summing up all points $p_i*pw_i + (\\alpha*w_j*c_j)$ and dividing by $\\sum_{i \\in j}(pw_i) + \\alpha*w_j$.\n",
    "5. Update cluster weights $w_j \\cdot \\alpha +\\sum_{i \\in j}(pw_i)$.\n",
    "6. Remove cluster if $w_j < \\delta$. Split largest cluster to replace it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ids-ws23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
