{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q4\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = [\n",
    "    [800, 30, 110],\n",
    "    [140, 50, 70 ],\n",
    "    [100, 20, 90 ]\n",
    "]\n",
    "conf_matrix = np.array(conf_matrix)\n",
    "features = ['monthly', 'quarterly', 'yearly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (a) low level\n",
    "def average_class_accuracy(conf_matrix):\n",
    "    # Apparently (refer Instruction 6) this is just the average of the recalls\n",
    "    # of each class\n",
    "    return sum(recall(i,conf_matrix) for i in range(len(features)))/len(features)\n",
    "\n",
    "def accuracy(conf_matrix):\n",
    "    # Accuracy = (TP+TN)/(TP+TN+FP+FN), but I don't think that can apply for\n",
    "    # a multiclass confusion matrix.\n",
    "    pass\n",
    "\n",
    "def recall(class_,conf_matrix):\n",
    "    # Recall = True Positive Rate, something I know how to do of the top of my hat\n",
    "    # but would have been completely lost in the exam because I didn't remember what recall\n",
    "    # is (ironic). All exams should be replaced with assignments they are not representetive\n",
    "    # of your abilities.\n",
    "    \n",
    "    # True positives on the diagonal\n",
    "    TP = conf_matrix[class_,class_]\n",
    "    # Positives in the row\n",
    "    P = sum(conf_matrix[class_])\n",
    "    return TP/P\n",
    "\n",
    "def precision(class_,conf_matrix):\n",
    "    # Wasn't asked but seems relevant\n",
    "    # True positives on the diagonal\n",
    "    TP = conf_matrix[class_,class_]\n",
    "    # False positives in the column\n",
    "    FP = sum(conf_matrix[:,class_]) - TP\n",
    "    return TP/(TP+FP)\n",
    "\n",
    "for i,v in enumerate(features):\n",
    "    print(f\"Recall for plan '{v}': {recall(i,conf_matrix):.2%}\")\n",
    "    print(f\"Precision for plan '{v}': {precision(i,conf_matrix):.2%}\")\n",
    "\n",
    "print(f\"Average class accuracy: {average_class_accuracy(conf_matrix):.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (b) data\n",
    "before = \"\"\"A B C plan\n",
    "1 2 1 monthly\n",
    "1 1 2 quarterly\n",
    "0 0 0 monthly\n",
    "1 0 0 quarterly\n",
    "0 1 0 quarterly\n",
    "1 2 1 monthly\n",
    "1 5 2 quarterly\n",
    "1 1 4 quarterly\n",
    "0 2 5 quarterly\n",
    "0 3 1 monthly\n",
    "1 2 1 monthly\n",
    "0 3 1 yearly\"\"\"\n",
    "after = \"\"\"A B C plan\n",
    "2 1 0 monthly\n",
    "1 2 1 quarterly\n",
    "0 3 0 monthly\n",
    "0 5 1 monthly\n",
    "0 0 6 quarterly\n",
    "0 1 1 yearly\n",
    "0 2 1 yearly\n",
    "1 5 2 yearly\n",
    "2 7 3 monthly\n",
    "1 8 4 quarterly\n",
    "0 5 5 monthly\"\"\"\n",
    "before = before.splitlines()\n",
    "\n",
    "columns = before[0].split()\n",
    "col_types = [int,int,int,str]\n",
    "\n",
    "before = [l.split() for l in before[1:]]\n",
    "before = pd.DataFrame(before, columns=columns)\n",
    "before = before.astype(dict(zip(columns,col_types)))\n",
    "\n",
    "after = after.splitlines()[1:]\n",
    "after = [l.split() for l in after]\n",
    "after = pd.DataFrame(after, columns=columns)\n",
    "after = after.astype(dict(zip(columns,col_types)))\n",
    "\n",
    "display(before.head())\n",
    "display(after.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (b) Dafuq is a system stability index?\n",
    "# Answer: A system stability index is (fraction of class before - fraction of class after) times the log of (fraction of class before/fraction of class after),\n",
    "# summed over all classes.\n",
    "\n",
    "# \"Compute the system stability index between before and after release\" <- Learn to write, I was like \"How do I do that before or after when it requires two\n",
    "# to compare?\". I think it means \"Compute the system stability index for the change from before to after release.\"\"\n",
    "\n",
    "def system_stability_index(before, after, summed=True):\n",
    "    ssi = []\n",
    "    ssi_plan = []\n",
    "    for plan in set(before['plan']) & set(after['plan']):\n",
    "        plan_percent_before = sum(before['plan'] == plan)/len(before)\n",
    "        plan_percent_after = sum(after['plan'] == plan)/len(after)\n",
    "        ssi.append((plan_percent_before - plan_percent_after) * np.log(plan_percent_before/plan_percent_after))\n",
    "        ssi_plan.append(plan)\n",
    "    if summed:\n",
    "        return sum(ssi)\n",
    "    else:\n",
    "        return zip(ssi_plan, ssi)\n",
    "\n",
    "print(f\"System stability index: {system_stability_index(before, after):.2f}\")\n",
    "for plan, ssi in system_stability_index(before, after, summed=False):\n",
    "    print(f\"System stability index for plan '{plan}': {ssi:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ids-ws23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
