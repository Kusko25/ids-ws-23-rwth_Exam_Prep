{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1]\n",
      "[2, 1, 1]\n",
      "2\n",
      "1.4142135623730951\n",
      "2.449489742783178\n",
      "0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity between documents\n",
    "# dotproduct(V1,V2)/(magnitude(V1)*magnitude(V2))\n",
    "# sum(V1_i*V2_i)/(sqrt(sum(V1_i^2))*sqrt(sum(V2_i^2))\n",
    "\n",
    "D1 = \"It is good weather.\"\n",
    "D1 = \"\".join(x for x in D1.lower() if x not in [\",\",\".\",\"!\"]).split()\n",
    "D2 = \"It is very very good weather.\"\n",
    "D2 = \"\".join(x for x in D2.lower() if x not in [\",\",\".\",\"!\"]).split()\n",
    "count_vector = [\"very\",\"good\",\"weather\"]\n",
    "D1_vector = [0,0,0]\n",
    "D2_vector = [0,0,0]\n",
    "for i in range(len(count_vector)):\n",
    "    D1_vector[i] = D1.count(count_vector[i])\n",
    "    D2_vector[i] = D2.count(count_vector[i])\n",
    "print(D1_vector)\n",
    "print(D2_vector)\n",
    "\n",
    "import math\n",
    "dot_product = 0\n",
    "for i in range(len(count_vector)):\n",
    "    dot_product += D1_vector[i]*D2_vector[i]\n",
    "print(dot_product)\n",
    "magnitude_D1 = math.sqrt(sum([x**2 for x in D1_vector]))\n",
    "magnitude_D2 = math.sqrt(sum([x**2 for x in D2_vector]))\n",
    "print(magnitude_D1)\n",
    "print(magnitude_D2)\n",
    "cosine_similarity = dot_product/(magnitude_D1*magnitude_D2)\n",
    "print(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['every', 'mystery', 'ever', 'solved', 'had', 'been', 'a', 'puzzle', 'from', 'the', 'dawn', 'of', 'the', 'human', 'specie', 'right', 'up', 'until', 'people', 'solved', 'it'], ['all', 'people', 'got', 'to', 'go', 'through', 'a', 'lot', 'of', 'flawed', 'idea', 'to', 'find', 'one', 'that', 'might', 'work', 'and', 'if', 'you', 'send', 'your', 'brain', 'negative', 'feedback', 'by', 'frowning', 'when', 'you', 'think', 'of', 'flawed', 'idea', 'pretty', 'soon', 'you', \"won't\", 'think', 'of', 'any', 'idea', 'at', 'all'], ['you', 'can', 'be', 'mistaken', 'about', 'what', 'you', 'believe', 'most', 'people', 'never', 'realize', \"there's\", 'a', 'difference', 'between', 'believing', 'an', 'idea', 'and', 'thinking', \"it's\", 'good', 'to', 'believe', 'it']]\n"
     ]
    }
   ],
   "source": [
    "# import lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "c = corpus = [\n",
    "    \"Every mystery ever solved had been a puzzle from the dawn of the human species right up until people solved it.\",\n",
    "    \"All people got to go through a lot of flawed ideas to find one that might work. And if you send your brain negative feedback by frowning when you think of flawed ideas, pretty soon you won't think of any ideas at all.\",\n",
    "    \"You can be mistaken about what you believe. Most people never realize there's a difference between believing an idea and thinking it's good to believe it.\"\n",
    "]\n",
    "lem = WordNetLemmatizer()\n",
    "for i,d in enumerate(corpus):\n",
    "    corpus[i] = \"\".join(x for x in d.lower() if x not in [\",\",\".\",\"!\"]).split()\n",
    "    corpus[i] = [lem.lemmatize(x) for x in corpus[i]]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def termfrequency(word,document):\n",
    "    count = sum(int(x==word) for x in document)\n",
    "    # count = 0\n",
    "    # for w in document:\n",
    "    #     if w==word:\n",
    "    #         count += 1\n",
    "    return count\n",
    "tf = termfrequency\n",
    "\n",
    "def termFrequencyInverseDocumentFrequency(word, document, corpus):\n",
    "    idf = len(corpus)/sum(word in doc for doc in corpus)\n",
    "    return termfrequency(word,document)*math.log2(idf)\n",
    "tfidf = termFrequencyInverseDocumentFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "1\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# tf idea\n",
    "# d1 ?\n",
    "# d2 ?\n",
    "# d3 ?\n",
    "query = \"idea\"\n",
    "for d in corpus:\n",
    "    print(tf(query,d))\n",
    "\n",
    "# tdidf people; df(people) = ?; idf(people); lb(?/?) = ?\n",
    "# d1    ?\n",
    "# d2    ?\n",
    "# d3    ?\n",
    "query = \"people\"\n",
    "for d in corpus:\n",
    "    print(tfidf(query,d,c))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ids-ws23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
